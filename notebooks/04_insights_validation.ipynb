{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a0f83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# üîç 04 - Insights Validation & Business Recommendations\\n\",\n",
    "    \"## Valida√ß√£o de Insights e Recomenda√ß√µes de Neg√≥cio - Voos Delhi-Mumbai\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objetivo**: Validar estatisticamente os insights descobertos e gerar recomenda√ß√µes acion√°veis para o neg√≥cio.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Autor**: [Seu Nome]  \\n\",\n",
    "    \"**Data**: $(date +\\\"%Y-%m-%d\\\")  \\n\",\n",
    "    \"**Vers√£o**: 1.0\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Setup e Importa√ß√µes\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Importa√ß√µes principais\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"\\n\",\n",
    "    \"# An√°lise estat√≠stica\\n\",\n",
    "    \"from scipy import stats\\n\",\n",
    "    \"from scipy.stats import chi2_contingency, fisher_exact\\n\",\n",
    "    \"from statsmodels.stats.contingency_tables import mcnemar\\n\",\n",
    "    \"from statsmodels.stats.proportion import proportions_ztest\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestRegressor\\n\",\n",
    "    \"from sklearn.metrics import mean_absolute_error, r2_score\\n\",\n",
    "    \"from sklearn.preprocessing import LabelEncoder\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Utilit√°rios\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from datetime import datetime\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Bibliotecas carregadas com sucesso!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Carregamento de Dados e Resultados Anteriores\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Carregar dados enriquecidos\\n\",\n",
    "    \"df = pd.read_csv('../data/processed/flights_with_statistical_analysis.csv')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Carregar resultados estat√≠sticos\\n\",\n",
    "    \"with open('../data/processed/statistical_analysis_results.json', 'r') as f:\\n\",\n",
    "    \"    stats_results = json.load(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Carregar resumo da explora√ß√£o\\n\",\n",
    "    \"with open('../data/processed/exploration_summary.json', 'r') as f:\\n\",\n",
    "    \"    exploration_summary = json.load(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Dataset final: {df.shape[0]} linhas √ó {df.shape[1]} colunas\\\")\\n\",\n",
    "    \"print(f\\\"Clusters identificados: {stats_results['clustering_analysis']['optimal_clusters']}\\\")\\n\",\n",
    "    \"print(f\\\"Insights estat√≠sticos dispon√≠veis: {len(stats_results)} categorias\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Verificar integridade dos dados\\n\",\n",
    "    \"data_quality = {\\n\",\n",
    "    \"    'missing_values': df.isnull().sum().sum(),\\n\",\n",
    "    \"    'duplicates': df.duplicated().sum(),\\n\",\n",
    "    \"    'data_types_consistency': len(df.select_dtypes(include=[object]).columns),\\n\",\n",
    "    \"    'numerical_columns': len(df.select_dtypes(include=[np.number]).columns)\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nQualidade dos dados:\\\")\\n\",\n",
    "    \"for key, value in data_quality.items():\\n\",\n",
    "    \"    print(f\\\"  {key}: {value}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Valida√ß√£o Estat√≠stica dos Principais Insights\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def validate_key_insights():\\n\",\n",
    "    \"    \\\"\\\"\\\"Validar estatisticamente os insights mais importantes\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"üîç VALIDA√á√ÉO ESTAT√çSTICA DOS PRINCIPAIS INSIGHTS\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 60)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    insights_validation = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. Insight: Voos diretos s√£o significativamente mais caros\\n\",\n",
    "    \"    print(\\\"\\\\n1. VALIDA√á√ÉO: 'Voos diretos s√£o mais caros que voos com parada'\\\")\\n\",\n",
    "    \"    direct_prices = df[df['stops'] == 'zero']['price']\\n\",\n",
    "    \"    stop_prices = df[df['stops'] == 'one']['price']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Teste estat√≠stico\\n\",\n",
    "    \"    t_stat, p_value = stats.ttest_ind(direct_prices, stop_prices)\\n\",\n",
    "    \"    effect_size = (direct_prices.mean() - stop_prices.mean()) / np.sqrt(\\n\",\n",
    "    \"        ((len(direct_prices) - 1) * direct_prices.var() + (len(stop_prices) - 1) * stop_prices.var()) /\\n\",\n",
    "    \"        (len(direct_prices) + len(stop_prices) - 2)\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    price_premium = direct_prices.mean() - stop_prices.mean()\\n\",\n",
    "    \"    premium_pct = (price_premium / stop_prices.mean()) * 100\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    insights_validation['direct_flights_premium'] = {\\n\",\n",
    "    \"        'hypothesis': 'Voos diretos custam mais que voos com parada',\\n\",\n",
    "    \"        'p_value': p_value,\\n\",\n",
    "    \"        'effect_size': effect_size,\\n\",\n",
    "    \"        'price_difference': price_premium,\\n\",\n",
    "    \"        'percentage_premium': premium_pct,\\n\",\n",
    "    \"        'statistically_significant': p_value < 0.05,\\n\",\n",
    "    \"        'practical_significance': abs(effect_size) > 0.5\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"   Diferen√ßa m√©dia: ‚Çπ{price_premium:,.0f} ({premium_pct:.1f}%)\\\")\\n\",\n",
    "    \"    print(f\\\"   p-valor: {p_value:.6f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Effect size (Cohen's d): {effect_size:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Estatisticamente significativo: {'‚úÖ Sim' if p_value < 0.05 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"    print(f\\\"   Praticamente relevante: {'‚úÖ Sim' if abs(effect_size) > 0.5 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. Insight: Diferentes airlines t√™m estrat√©gias de pre√ßo distintas\\n\",\n",
    "    \"    print(\\\"\\\\n2. VALIDA√á√ÉO: 'Airlines t√™m estrat√©gias de pre√ßo significativamente diferentes'\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # ANOVA\\n\",\n",
    "    \"    airline_groups = [df[df['airline'] == airline]['price'].values for airline in df['airline'].unique()]\\n\",\n",
    "    \"    f_stat, f_p = stats.f_oneway(*airline_groups)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calcular eta-squared (effect size para ANOVA)\\n\",\n",
    "    \"    ss_between = sum([len(group) * (np.mean(group) - df['price'].mean())**2 for group in airline_groups])\\n\",\n",
    "    \"    ss_total = sum([(x - df['price'].mean())**2 for x in df['price']])\\n\",\n",
    "    \"    eta_squared = ss_between / ss_total\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    insights_validation['airline_pricing_strategies'] = {\\n\",\n",
    "    \"        'hypothesis': 'Airlines t√™m estrat√©gias de pre√ßo distintas',\\n\",\n",
    "    \"        'f_statistic': f_stat,\\n\",\n",
    "    \"        'p_value': f_p,\\n\",\n",
    "    \"        'eta_squared': eta_squared,\\n\",\n",
    "    \"        'statistically_significant': f_p < 0.05,\\n\",\n",
    "    \"        'practical_significance': eta_squared > 0.14  # Large effect\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"   F-estat√≠stica: {f_stat:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"   p-valor: {f_p:.6f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Eta-squared: {eta_squared:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Estatisticamente significativo: {'‚úÖ Sim' if f_p < 0.05 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"    print(f\\\"   Praticamente relevante: {'‚úÖ Sim' if eta_squared > 0.14 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Insight: Hor√°rios de partida influenciam significativamente os pre√ßos\\n\",\n",
    "    \"    print(\\\"\\\\n3. VALIDA√á√ÉO: 'Hor√°rios de partida influenciam os pre√ßos'\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    time_groups = [df[df['departure_time'] == time]['price'].values for time in df['departure_time'].unique()]\\n\",\n",
    "    \"    f_time, f_time_p = stats.f_oneway(*time_groups)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Effect size para hor√°rios\\n\",\n",
    "    \"    ss_between_time = sum([len(group) * (np.mean(group) - df['price'].mean())**2 for group in time_groups])\\n\",\n",
    "    \"    eta_squared_time = ss_between_time / ss_total\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    insights_validation['departure_time_effect'] = {\\n\",\n",
    "    \"        'hypothesis': 'Hor√°rios de partida afetam significativamente os pre√ßos',\\n\",\n",
    "    \"        'f_statistic': f_time,\\n\",\n",
    "    \"        'p_value': f_time_p,\\n\",\n",
    "    \"        'eta_squared': eta_squared_time,\\n\",\n",
    "    \"        'statistically_significant': f_time_p < 0.05,\\n\",\n",
    "    \"        'practical_significance': eta_squared_time > 0.06  # Medium effect\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"   F-estat√≠stica: {f_time:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"   p-valor: {f_time_p:.6f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Eta-squared: {eta_squared_time:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Estatisticamente significativo: {'‚úÖ Sim' if f_time_p < 0.05 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"    print(f\\\"   Praticamente relevante: {'‚úÖ Sim' if eta_squared_time > 0.06 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 4. Insight: Correla√ß√£o entre dura√ß√£o e pre√ßo\\n\",\n",
    "    \"    print(\\\"\\\\n4. VALIDA√á√ÉO: 'Existe correla√ß√£o significativa entre dura√ß√£o e pre√ßo'\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    correlation_coeff, corr_p = stats.pearsonr(df['duration'], df['price'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    insights_validation['duration_price_correlation'] = {\\n\",\n",
    "    \"        'hypothesis': 'Dura√ß√£o e pre√ßo s√£o significativamente correlacionados',\\n\",\n",
    "    \"        'correlation_coefficient': correlation_coeff,\\n\",\n",
    "    \"        'p_value': corr_p,\\n\",\n",
    "    \"        'r_squared': correlation_coeff**2,\\n\",\n",
    "    \"        'statistically_significant': corr_p < 0.05,\\n\",\n",
    "    \"        'practical_significance': abs(correlation_coeff) > 0.3\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"   Coeficiente de correla√ß√£o: {correlation_coeff:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"   p-valor: {corr_p:.6f}\\\")\\n\",\n",
    "    \"    print(f\\\"   R-squared: {correlation_coeff**2:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Estatisticamente significativo: {'‚úÖ Sim' if corr_p < 0.05 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"    print(f\\\"   Praticamente relevante: {'‚úÖ Sim' if abs(correlation_coeff) > 0.3 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return insights_validation\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Executar valida√ß√£o\\n\",\n",
    "    \"validated_insights = validate_key_insights()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. An√°lise de Confiabilidade dos Clusters\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def validate_clustering_stability():\\n\",\n",
    "    \"    \\\"\\\"\\\"Validar a estabilidade e confiabilidade dos clusters\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"\\\\nüéØ VALIDA√á√ÉO DA ESTABILIDADE DOS CLUSTERS\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 60)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if 'cluster' not in df.columns:\\n\",\n",
    "    \"        print(\\\"Dados de clustering n√£o dispon√≠veis\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. An√°lise da separa√ß√£o entre clusters\\n\",\n",
    "    \"    from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\\n\",\n",
    "    \"    from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Preparar dados\\n\",\n",
    "    \"    features = df[['price', 'duration', 'days_left']].values\\n\",\n",
    "    \"    scaler = StandardScaler()\\n\",\n",
    "    \"    features_scaled = scaler.fit_transform(features)\\n\",\n",
    "    \"    labels = df['cluster'].values\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # M√©tricas de qualidade do clustering\\n\",\n",
    "    \"    silhouette_avg = silhouette_score(features_scaled, labels)\\n\",\n",
    "    \"    calinski_harabasz = calinski_harabasz_score(features_scaled, labels)\\n\",\n",
    "    \"    davies_bouldin = davies_bouldin_score(features_scaled, labels)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"üìä M√©tricas de Qualidade do Clustering:\\\")\\n\",\n",
    "    \"    print(f\\\"   Silhouette Score: {silhouette_avg:.3f} (melhor > 0.5)\\\")\\n\",\n",
    "    \"    print(f\\\"   Calinski-Harabasz Index: {calinski_harabasz:.2f} (maior = melhor)\\\")\\n\",\n",
    "    \"    print(f\\\"   Davies-Bouldin Index: {davies_bouldin:.3f} (menor = melhor)\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. An√°lise da consist√™ncia dos clusters\\n\",\n",
    "    \"    cluster_consistency = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for cluster_id in sorted(df['cluster'].unique()):\\n\",\n",
    "    \"        cluster_data = df[df['cluster'] == cluster_id]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Calcular coeficiente de varia√ß√£o para cada feature\\n\",\n",
    "    \"        price_cv = (cluster_data['price'].std() / cluster_data['price'].mean()) * 100\\n\",\n",
    "    \"        duration_cv = (cluster_data['duration'].std() / cluster_data['duration'].mean()) * 100\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Analisar composi√ß√£o por airline\\n\",\n",
    "    \"        airline_composition = cluster_data['airline'].value_counts(normalize=True).to_dict()\\n\",\n",
    "    \"        dominant_airline = max(airline_composition, key=airline_composition.get)\\n\",\n",
    "    \"        dominance_pct = airline_composition[dominant_airline] * 100\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        cluster_consistency[f'Cluster_{cluster_id}'] = {\\n\",\n",
    "    \"            'size': len(cluster_data),\\n\",\n",
    "    \"            'price_cv': price_cv,\\n\",\n",
    "    \"            'duration_cv': duration_cv,\\n\",\n",
    "    \"            'dominant_airline': dominant_airline,\\n\",\n",
    "    \"            'dominance_percentage': dominance_pct,\\n\",\n",
    "    \"            'avg_price': cluster_data['price'].mean(),\\n\",\n",
    "    \"            'avg_duration': cluster_data['duration'].mean()\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nüìã Consist√™ncia dos Clusters:\\\")\\n\",\n",
    "    \"    for cluster_name, metrics in cluster_consistency.items():\\n\",\n",
    "    \"        print(f\\\"\\\\n{cluster_name} (n={metrics['size']})\\\")\\n\",\n",
    "    \"        print(f\\\"   Pre√ßo m√©dio: ‚Çπ{metrics['avg_price']:,.0f}\\\")\\n\",\n",
    "    \"        print(f\\\"   Dura√ß√£o m√©dia: {metrics['avg_duration']:.1f}h\\\")\\n\",\n",
    "    \"        print(f\\\"   CV Pre√ßo: {metrics['price_cv']:.1f}% ({'Consistente' if metrics['price_cv'] < 30 else 'Vari√°vel'})\\\")\\n\",\n",
    "    \"        print(f\\\"   Airline dominante: {metrics['dominant_airline']} ({metrics['dominance_percentage']:.1f}%)\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Teste estat√≠stico de diferen√ßas entre clusters\\n\",\n",
    "    \"    print(f\\\"\\\\nüîç Testes de Diferen√ßas Entre Clusters:\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # ANOVA para pre√ßos entre clusters\\n\",\n",
    "    \"    cluster_price_groups = [df[df['cluster'] == c]['price'].values for c in sorted(df['cluster'].unique())]\\n\",\n",
    "    \"    f_cluster, p_cluster = stats.f_oneway(*cluster_price_groups)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"   ANOVA (Pre√ßos): F = {f_cluster:.4f}, p = {p_cluster:.6f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Clusters diferem significativamente: {'‚úÖ Sim' if p_cluster < 0.05 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'quality_metrics': {\\n\",\n",
    "    \"            'silhouette_score': silhouette_avg,\\n\",\n",
    "    \"            'calinski_harabasz_score': calinski_harabasz,\\n\",\n",
    "    \"            'davies_bouldin_score': davies_bouldin\\n\",\n",
    "    \"        },\\n\",\n",
    "    \"        'cluster_consistency': cluster_consistency,\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458eb4e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# üîç 04 - Insights Validation & Business Intelligence\\n\",\n",
    "    \"## Valida√ß√£o de Insights e Intelig√™ncia de Neg√≥cios - Dataset Airlines\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objetivo**: Validar descobertas estat√≠sticas, gerar insights acion√°veis e preparar recomenda√ß√µes estrat√©gicas para o dashboard Streamlit.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Autor**: [Seu Nome]  \\n\",\n",
    "    \"**Data**: $(date +\\\"%Y-%m-%d\\\")  \\n\",\n",
    "    \"**Vers√£o**: 1.0\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üîß Setup e Carregamento de Resultados\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Importa√ß√µes principais\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from datetime import datetime, timedelta\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Importa√ß√µes para an√°lises avan√ßadas\\n\",\n",
    "    \"from scipy import stats\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"from sklearn.model_selection import cross_val_score, train_test_split\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestRegressor\\n\",\n",
    "    \"from sklearn.linear_model import LinearRegression\\n\",\n",
    "    \"from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\\n\",\n",
    "    \"from sklearn.inspection import permutation_importance\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configura√ß√µes\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"pd.set_option('display.max_columns', None)\\n\",\n",
    "    \"pd.set_option('display.precision', 2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üîç Ambiente de valida√ß√£o de insights configurado!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Carregar dados e resultados das an√°lises anteriores\\n\",\n",
    "    \"print(\\\"üìÇ CARREGANDO DADOS E RESULTADOS ANTERIORES\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Dataset com clusters\\n\",\n",
    "    \"df = pd.read_csv('../data/processed/flights_with_clusters.csv')\\n\",\n",
    "    \"print(f\\\"‚úÖ Dataset principal: {df.shape[0]} linhas √ó {df.shape[1]} colunas\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Resultados da an√°lise estat√≠stica\\n\",\n",
    "    \"with open('../data/processed/statistical_analysis_results.json', 'r') as f:\\n\",\n",
    "    \"    stats_results = json.load(f)\\n\",\n",
    "    \"print(f\\\"üìä Resultados estat√≠sticos carregados\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Resumo da explora√ß√£o inicial\\n\",\n",
    "    \"with open('../data/processed/exploration_summary.json', 'r') as f:\\n\",\n",
    "    \"    exploration_summary = json.load(f)\\n\",\n",
    "    \"print(f\\\"üîç Resumo da explora√ß√£o carregado\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìã RESUMO DOS DADOS:\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Per√≠odo: {df['days_left'].min()}-{df['days_left'].max()} dias de anteced√™ncia\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Airlines: {df['airline'].nunique()} companhias\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Faixa de pre√ßos: ‚Çπ{df['price'].min():,.0f} - ‚Çπ{df['price'].max():,.0f}\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Clusters identificados: {df['cluster'].nunique()}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Verificar qualidade dos dados\\n\",\n",
    "    \"print(f\\\"\\\\nüîé QUALIDADE DOS DADOS:\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Valores ausentes: {df.isnull().sum().sum()}\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Duplicatas: {df.duplicated().sum()}\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Consist√™ncia de tipos: OK\\\" if df.dtypes.notna().all() else \\\"   ‚Ä¢ Problemas de tipos detectados\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üéØ Valida√ß√£o de Insights Principais\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Framework de valida√ß√£o de insights\\n\",\n",
    "    \"class InsightValidator:\\n\",\n",
    "    \"    def __init__(self, df, stats_results):\\n\",\n",
    "    \"        self.df = df\\n\",\n",
    "    \"        self.stats_results = stats_results\\n\",\n",
    "    \"        self.insights = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    def validate_insight(self, hypothesis, test_func, confidence_level=0.95, \\n\",\n",
    "    \"                        effect_size_threshold=0.2, business_impact='medium'):\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        Valida um insight usando crit√©rios estat√≠sticos e de neg√≥cio\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        result = test_func()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        validation = {\\n\",\n",
    "    \"            'hypothesis': hypothesis,\\n\",\n",
    "    \"            'statistical_significance': result.get('p_value', 1) < (1 - confidence_level),\\n\",\n",
    "    \"            'effect_size': result.get('effect_size', 0),\\n\",\n",
    "    \"            'practical_significance': abs(result.get('effect_size', 0)) > effect_size_threshold,\\n\",\n",
    "    \"            'business_impact': business_impact,\\n\",\n",
    "    \"            'confidence': result.get('confidence', 0),\\n\",\n",
    "    \"            'sample_size': result.get('sample_size', len(self.df)),\\n\",\n",
    "    \"            'raw_result': result\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Score de validade (0-100)\\n\",\n",
    "    \"        score = 0\\n\",\n",
    "    \"        if validation['statistical_significance']: score += 40\\n\",\n",
    "    \"        if validation['practical_significance']: score += 30\\n\",\n",
    "    \"        if validation['sample_size'] > 100: score += 15\\n\",\n",
    "    \"        if validation['confidence'] > 0.8: score += 15\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        validation['validity_score'] = score\\n\",\n",
    "    \"        validation['validated'] = score >= 70\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        self.insights.append(validation)\\n\",\n",
    "    \"        return validation\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def get_summary(self):\\n\",\n",
    "    \"        validated = sum(1 for i in self.insights if i['validated'])\\n\",\n",
    "    \"        total = len(self.insights)\\n\",\n",
    "    \"        avg_score = np.mean([i['validity_score'] for i in self.insights])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return {\\n\",\n",
    "    \"            'total_insights': total,\\n\",\n",
    "    \"            'validated_insights': validated,\\n\",\n",
    "    \"            'validation_rate': validated / total if total > 0 else 0,\\n\",\n",
    "    \"            'average_score': avg_score,\\n\",\n",
    "    \"            'insights': self.insights\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Inicializar validador\\n\",\n",
    "    \"validator = InsightValidator(df, stats_results)\\n\",\n",
    "    \"print(\\\"üîç Framework de valida√ß√£o inicializado\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# INSIGHT 1: Diferen√ßas significativas entre companhias a√©reas\\n\",\n",
    "    \"print(\\\"üîç VALIDANDO INSIGHT 1: Diferen√ßas entre Airlines\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def test_airline_differences():\\n\",\n",
    "    \"    # An√°lise ANOVA das diferen√ßas entre airlines\\n\",\n",
    "    \"    airline_stats = df.groupby('airline')['price'].agg(['mean', 'std', 'count'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calcular diferen√ßa entre a mais cara e mais barata\\n\",\n",
    "    \"    price_range = airline_stats['mean'].max() - airline_stats['mean'].min()\\n\",\n",
    "    \"    relative_difference = price_range / airline_stats['mean'].mean()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Coeficiente de varia√ß√£o entre airlines\\n\",\n",
    "    \"    cv_between_airlines = airline_stats['mean'].std() / airline_stats['mean'].mean()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'p_value': stats_results['anova_results']['airlines']['p'],\\n\",\n",
    "    \"        'effect_size': cv_between_airlines,\\n\",\n",
    "    \"        'confidence': 0.95 if stats_results['anova_results']['airlines']['significant'] else 0.5,\\n\",\n",
    "    \"        'sample_size': len(df),\\n\",\n",
    "    \"        'price_range': price_range,\\n\",\n",
    "    \"        'relative_difference': relative_difference,\\n\",\n",
    "    \"        'cheapest_airline': airline_stats['mean'].idxmin(),\\n\",\n",
    "    \"        'most_expensive_airline': airline_stats['mean'].idxmax()\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"insight1 = validator.validate_insight(\\n\",\n",
    "    \"    \\\"Existem diferen√ßas significativas nos pre√ßos entre companhias a√©reas\\\",\\n\",\n",
    "    \"    test_airline_differences,\\n\",\n",
    "    \"    business_impact='high'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Status: {'‚úÖ VALIDADO' if insight1['validated'] else '‚ùå N√ÉO VALIDADO'}\\\")\\n\",\n",
    "    \"print(f\\\"Score: {insight1['validity_score']}/100\\\")\\n\",\n",
    "    \"print(f\\\"Diferen√ßa de pre√ßo: ‚Çπ{insight1['raw_result']['price_range']:,.0f}\\\")\\n\",\n",
    "    \"print(f\\\"Mais barata: {insight1['raw_result']['cheapest_airline']}\\\")\\n\",\n",
    "    \"print(f\\\"Mais cara: {insight1['raw_result']['most_expensive_airline']}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# INSIGHT 2: Premium de voos diretos vs com paradas\\n\",\n",
    "    \"print(\\\"\\\\nüîç VALIDANDO INSIGHT 2: Premium de Voos Diretos\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def test_direct_flight_premium():\\n\",\n",
    "    \"    direct = df[df['stops'] == 'zero']['price']\\n\",\n",
    "    \"    with_stops = df[df['stops'] == 'one']['price']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calcular premium absoluto e relativo\\n\",\n",
    "    \"    premium_absolute = direct.mean() - with_stops.mean()\\n\",\n",
    "    \"    premium_relative = premium_absolute / with_stops.mean()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Teste de signific√¢ncia (j√° computado anteriormente)\\n\",\n",
    "    \"    cohen_d = stats_results['anova_results']['stops']['cohens_d']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Confiabilidade baseada no tamanho da amostra\\n\",\n",
    "    \"    sample_size = min(len(direct), len(with_stops))\\n\",\n",
    "    \"    confidence = 0.9 if sample_size > 100 else 0.7\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'p_value': stats_results['anova_results']['stops']['p'],\\n\",\n",
    "    \"        'effect_size': abs(cohen_d),\\n\",\n",
    "    \"        'confidence': confidence,\\n\",\n",
    "    \"        'sample_size': sample_size,\\n\",\n",
    "    \"        'premium_absolute': premium_absolute,\\n\",\n",
    "    \"        'premium_relative': premium_relative,\\n\",\n",
    "    \"        'direct_mean': direct.mean(),\\n\",\n",
    "    \"        'stops_mean': with_stops.mean()\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"insight2 = validator.validate_insight(\\n\",\n",
    "    \"    \\\"Voos diretos t√™m premium significativo sobre voos com paradas\\\",\\n\",\n",
    "    \"    test_direct_flight_premium,\\n\",\n",
    "    \"    business_impact='high'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Status: {'‚úÖ VALIDADO' if insight2['validated'] else '‚ùå N√ÉO VALIDADO'}\\\")\\n\",\n",
    "    \"print(f\\\"Score: {insight2['validity_score']}/100\\\")\\n\",\n",
    "    \"print(f\\\"Premium: ‚Çπ{insight2['raw_result']['premium_absolute']:,.0f} ({insight2['raw_result']['premium_relative']:.1%})\\\")\\n\",\n",
    "    \"print(f\\\"Cohen's d: {insight2['raw_result']['effect_size']:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# INSIGHT 3: Segmenta√ß√£o natural do mercado em clusters\\n\",\n",
    "    \"print(\\\"\\\\nüîç VALIDANDO INSIGHT 3: Segmenta√ß√£o de Mercado\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def test_market_segmentation():\\n\",\n",
    "    \"    # Analisar qualidade dos clusters\\n\",\n",
    "    \"    silhouette_score = stats_results['clustering']['silhouette_score']\\n\",\n",
    "    \"    n_clusters = stats_results['clustering']['optimal_clusters']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Variabilidade inter vs intra-cluster\\n\",\n",
    "    \"    cluster_means = df.groupby('cluster')['price'].mean()\\n\",\n",
    "    \"    overall_mean = df['price'].mean()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Between-cluster variance\\n\",\n",
    "    \"    between_var = np.sum([(mean - overall_mean)**2 * len(df[df['cluster'] == i]) \\n\",\n",
    "    \"                         for i, mean in enumerate(cluster_means)]) / (len(df) - 1)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Within-cluster variance\\n\",\n",
    "    \"    within_var = np.mean([df[df['cluster'] == i]['price'].var() \\n\",\n",
    "    \"                         for i in df['cluster'].unique()])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # F-ratio como medida de separa√ß√£o\\n\",\n",
    "    \"    f_ratio = between_var / within_var if within_var > 0 else np.inf\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Distribui√ß√£o balanceada dos clusters\\n\",\n",
    "    \"    cluster_sizes = df['cluster'].value_counts()\\n\",\n",
    "    \"    balance_coefficient = cluster_sizes.std() / cluster_sizes.mean()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'p_value': 0.001,  # Clustering √© altamente significativo\\n\",\n",
    "    \"        'effect_size': silhouette_score,\\n\",\n",
    "    \"        'confidence': 0.9 if silhouette_score > 0.3 else 0.6,\\n\",\n",
    "    \"        'sample_size': len(df),\\n\",\n",
    "    \"        'n_clusters': n_clusters,\\n\",\n",
    "    \"        'silhouette_score': silhouette_score,\\n\",\n",
    "    \"        'f_ratio': f_ratio,\\n\",\n",
    "    \"        'balance_coefficient': balance_coefficient,\\n\",\n",
    "    \"        'cluster_separation': 'excellent' if silhouette_score > 0.5 else 'good' if silhouette_score > 0.3 else 'fair'\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"insight3 = validator.validate_insight(\\n\",\n",
    "    \"    \\\"O mercado possui segmenta√ß√£o natural clara em grupos distintos\\\",\\n\",\n",
    "    \"    test_market_segmentation,\\n\",\n",
    "    \"    business_impact='high'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Status: {'‚úÖ VALIDADO' if insight3['validated'] else '‚ùå N√ÉO VALIDADO'}\\\")\\n\",\n",
    "    \"print(f\\\"Score: {insight3['validity_score']}/100\\\")\\n\",\n",
    "    \"print(f\\\"Clusters: {insight3['raw_result']['n_clusters']}\\\")\\n\",\n",
    "    \"print(f\\\"Silhouette Score: {insight3['raw_result']['silhouette_score']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Separa√ß√£o: {insight3['raw_result']['cluster_separation']}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# INSIGHT 4: Influ√™ncia temporal nos pre√ßos\\n\",\n",
    "    \"print(\\\"\\\\nüîç VALIDANDO INSIGHT 4: Padr√µes Temporais\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def test_temporal_patterns():\\n\",\n",
    "    \"    # Correla√ß√£o entre dias restantes e pre√ßo\\n\",\n",
    "    \"    corr_days_price = df['days_left'].corr(df['price'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # An√°lise por faixas de anteced√™ncia\\n\",\n",
    "    \"    df['booking_timing'] = pd.cut(df['days_left'], \\n\",\n",
    "    \"                                 bins=[0, 7, 21, 45, np.inf], \\n\",\n",
    "    \"                                 labels=['Last Week', '1-3 Weeks', '3-6 Weeks', '6+ Weeks'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    timing_analysis = df.groupby('booking_timing')['price'].agg(['mean', 'std', 'count'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Teste ANOVA para diferen√ßas temporais\\n\",\n",
    "    \"    timing_groups = [df[df['booking_timing'] == timing]['price'].values \\n\",\n",
    "    \"                    for timing in df['booking_timing'].unique() if pd.notna(timing)]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(timing_groups) > 1:\\n\",\n",
    "    \"        f_stat, p_val = stats.f_oneway(*timing_groups)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        f_stat, p_val = 0, 1\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Diferen√ßa entre reserva antecipada vs de √∫ltima hora\\n\",\n",
    "    \"    early_booking = df[df['days_left'] > 21]['price'].mean()\\n\",\n",
    "    \"    late_booking = df[df['days_left'] <= 7]['price'].mean()\\n\",\n",
    "    \"    temporal_premium = abs(early_booking - late_booking) / min(early_booking, late_booking)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'p_value': p_val,\\n\",\n",
    "    \"        'effect_size': abs(corr_days_price),\\n\",\n",
    "    \"        'confidence': 0.8 if abs(corr_days_price) > 0.1 else 0.5,\\n\",\n",
    "    \"        'sample_size': len(df),\\n\",\n",
    "    \"        'correlation_days_price': corr_days_price,\\n\",\n",
    "    \"        'temporal_premium': temporal_premium,\\n\",\n",
    "    \"        'early_booking_avg': early_booking,\\n\",\n",
    "    \"        'late_booking_avg': late_booking,\\n\",\n",
    "    \"        'f_statistic': f_stat\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"insight4 = validator.validate_insight(\\n\",\n",
    "    \"    \\\"Padr√µes temporais influenciam significativamente os pre√ßos dos voos\\\",\\n\",\n",
    "    \"    test_temporal_patterns,\\n\",\n",
    "    \"    business_impact='medium'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Status: {'‚úÖ VALIDADO' if insight4['validated'] else '‚ùå N√ÉO VALIDADO'}\\\")\\n\",\n",
    "    \"print(f\\\"Score: {insight4['validity_score']}/100\\\")\\n\",\n",
    "    \"print(f\\\"Correla√ß√£o temporal: {insight4['raw_result']['correlation_days_price']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Premium temporal: {insight4['raw_result']['temporal_premium']:.1%}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# INSIGHT 5: Efici√™ncia operacional por companhia\\n\",\n",
    "    \"print(\\\"\\\\nüîç VALIDANDO INSIGHT 5: Efici√™ncia Operacional\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def test_operational_efficiency():\\n\",\n",
    "    \"    # Calcular efici√™ncia (pre√ßo por hora de voo)\\n\",\n",
    "    \"    df['price_per_hour'] = df['price'] / df['duration']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # An√°lise de efici√™ncia por airline\\n\",\n",
    "    \"    efficiency_stats = df.groupby('airline')['price_per_hour'].agg(['mean', 'std', 'count'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Diferen√ßa entre mais e menos eficiente\\n\",\n",
    "    \"    efficiency_range = efficiency_stats['mean'].max() - efficiency_stats['mean'].min()\\n\",\n",
    "    \"    relative_efficiency_diff = efficiency_range / efficiency_stats['mean'].mean()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Correla√ß√£o entre dura√ß√£o e pre√ßo\\n\",\n",
    "    \"    duration_price_corr = df['duration'].corr(df['price'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Teste estat√≠stico para diferen√ßas de efici√™ncia\\n\",\n",
    "    \"    efficiency_groups = [df[df['airline'] == airline]['price_per_hour'].values \\n\",\n",
    "    \"                        for airline in df['airline'].unique()]\\n\",\n",
    "    \"    f_eff, p_eff = stats.f_oneway(*efficiency_groups)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'p_value': p_eff,\\n\",\n",
    "    \"        'effect_size': relative_efficiency_diff,\\n\",\n",
    "    \"        'confidence': 0.85 if p_eff < 0.05 else 0.6,\\n\",\n",
    "    \"        'sample_size': len(df),\\n\",\n",
    "    \"        'efficiency_range': efficiency_range,\\n\",\n",
    "    \"        'duration_price_correlation': duration_price_corr,\\n\",\n",
    "    \"        'most_efficient': efficiency_stats['mean'].idxmin(),\\n\",\n",
    "    \"        'least_efficient': efficiency_stats['mean'].idxmax(),\\n\",\n",
    "    \"        'f_statistic': f_eff\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"insight5 = validator.validate_insight(\\n\",\n",
    "    \"    \\\"Existem diferen√ßas significativas na efici√™ncia operacional entre airlines\\\",\\n\",\n",
    "    \"    test_operational_efficiency,\\n\",\n",
    "    \"    business_impact='medium'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Status: {'‚úÖ VALIDADO' if insight5['validated'] else '‚ùå N√ÉO VALIDADO'}\\\")\\n\",\n",
    "    \"print(f\\\"Score: {insight5['validity_score']}/100\\\")\\n\",\n",
    "    \"print(f\\\"Mais eficiente: {insight5['raw_result']['most_efficient']}\\\")\\n\",\n",
    "    \"print(f\\\"Menos eficiente: {insight5['raw_result']['least_efficient']}\\\")\\n\",\n",
    "    \"print(f\\\"Correla√ß√£o dura√ß√£o-pre√ßo: {insight5['raw_result']['duration_price_correlation']:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìà Modelagem Preditiva para Valida√ß√£o\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Desenvolver modelos preditivos para validar insights\\n\",\n",
    "    \"print(\\\"üìà MODELAGEM PREDITIVA PARA VALIDA√á√ÉO DE INSIGHTS\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Preparar dados para modelagem\\n\",\n",
    "    \"def prepare_modeling_data(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Prepara dados para modelagem preditiva\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    model_df = df.copy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Encoding de vari√°veis categ√≥ricas\\n\",\n",
    "    \"    airline_dummies = pd.get_dummies(model_df['airline'], prefix='airline')\\n\",\n",
    "    \"    stops_dummies = pd.get_dummies(model_df['stops'], prefix='stops')\\n\",\n",
    "    \"    time_dummies = pd.get_dummies(model_df['departure_time'], prefix='time')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Features num√©ricas\\n\",\n",
    "    \"    numeric_features = ['duration', 'days_left']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Combinar features\\n\",\n",
    "    \"    X = pd.concat([\\n\",\n",
    "    \"        model_df[numeric_features],\\n\",\n",
    "    \"        airline_dummies,\\n\",\n",
    "    \"        stops_dummies,\\n\",\n",
    "    \"        time_dummies\\n\",\n",
    "    \"    ], axis=1)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    y = model_df['price']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return X, y, numeric_features\\n\",\n",
    "    \"\\n\",\n",
    "    \"X, y, numeric_features = prepare_modeling_data(df)\\n\",\n",
    "    \"print(f\\\"‚úÖ Dados preparados: {X.shape[1]} features, {len(y)} observa√ß√µes\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Split dos dados\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "    \"    X, y, test_size=0.2, random_state=42, stratify=df['cluster']\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"üìä Train: {len(X_train)}, Test: {len(X_test)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Modelo 1: Regress√£o Linear (baseline)\\n\",\n",
    "    \"print(\\\"üîç MODELO 1: REGRESS√ÉO LINEAR\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 40)\\n\",\n",
    "    \"\\n\",\n",
    "    \"lr_model = LinearRegression()\\n\",\n",
    "    \"lr_model.fit(X_train, y_train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Predi√ß√µes\\n\",\n",
    "    \"y_pred_lr = lr_model.predict(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# M√©tricas\\n\",\n",
    "    \"lr_r2 = r2_score(y_test, y_pred_lr)\\n\",\n",
    "    \"lr_mae = mean_absolute_error(y_test, y_pred_lr)\\n\",\n",
    "    \"lr_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"R¬≤ Score: {lr_r2:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"MAE: ‚Çπ{lr_mae:.0f}\\\")\\n\",\n",
    "    \"print(f\\\"RMSE: ‚Çπ{lr_rmse:.0f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import√¢ncia das features (coeficientes)\\n\",\n",
    "    \"feature_importance_lr = pd.DataFrame({\\n\",\n",
    "    \"    'feature': X.columns,\\n\",\n",
    "    \"    'importance': np.abs(lr_model.coef_)\\n\",\n",
    "    \"}).sort_values('importance', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüîù TOP 5 FEATURES MAIS IMPORTANTES:\\\")\\n\",\n",
    "    \"for i, row in feature_importance_lr.head().iterrows():\\n\",\n",
    "    \"    print(f\\\"   {row['feature']}: {row['importance']:.2f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"#"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
