{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3cbcd5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# üìà 02 - Statistical Analysis: Airlines Dataset\\n\",\n",
    "    \"## An√°lise Estat√≠stica Inferencial - Voos Delhi-Mumbai\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objetivo**: Realizar testes estat√≠sticos, an√°lises de correla√ß√£o e infer√™ncias sobre os padr√µes identificados na explora√ß√£o inicial.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Autor**: [Seu Nome]  \\n\",\n",
    "    \"**Data**: $(date +\\\"%Y-%m-%d\\\")  \\n\",\n",
    "    \"**Vers√£o**: 1.0\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üîß Setup e Importa√ß√µes\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Importa√ß√µes principais\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Importa√ß√µes estat√≠sticas\\n\",\n",
    "    \"from scipy import stats\\n\",\n",
    "    \"from scipy.stats import (\\n\",\n",
    "    \"    normaltest, shapiro, levene, bartlett, \\n\",\n",
    "    \"    ttest_ind, mannwhitneyu, f_oneway, kruskal,\\n\",\n",
    "    \"    chi2_contingency, pearsonr, spearmanr,\\n\",\n",
    "    \"    jarque_bera, anderson\\n\",\n",
    "    \")\\n\",\n",
    "    \"from statsmodels.stats.multicomp import pairwise_tukeyhsd\\n\",\n",
    "    \"from statsmodels.stats.diagnostic import het_breuschpagan\\n\",\n",
    "    \"from statsmodels.formula.api import ols\\n\",\n",
    "    \"import statsmodels.api as sm\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"from sklearn.cluster import KMeans\\n\",\n",
    "    \"from sklearn.metrics import silhouette_score\\n\",\n",
    "    \"\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configura√ß√µes\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"pd.set_option('display.max_columns', None)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üì¶ Todas as bibliotecas estat√≠sticas carregadas com sucesso!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üì• Carregamento dos Dados\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Carregar dados processados do notebook anterior\\n\",\n",
    "    \"df = pd.read_csv('../data/processed/cleaned_flights_data.csv')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úÖ Dataset carregado: {df.shape[0]} linhas √ó {df.shape[1]} colunas\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Carregar resumo da explora√ß√£o\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"with open('../data/processed/exploration_summary.json', 'r') as f:\\n\",\n",
    "    \"    exploration_summary = json.load(f)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"print(f\\\"üìä Resumo da explora√ß√£o carregado\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Pre√ßo m√©dio anterior: ‚Çπ{exploration_summary['price_stats']['mean']:.0f}\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Voos diretos: {exploration_summary['direct_flights_pct']:.1f}%\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìä Testes de Normalidade\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Fun√ß√£o para testes de normalidade\\n\",\n",
    "    \"def test_normality(data, variable_name, alpha=0.05):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Testa normalidade usando m√∫ltiplos testes\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"üîç TESTE DE NORMALIDADE - {variable_name.upper()}\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 50)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Remover valores nulos\\n\",\n",
    "    \"    clean_data = data.dropna()\\n\",\n",
    "    \"    n = len(clean_data)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    results = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Shapiro-Wilk (melhor para n < 5000)\\n\",\n",
    "    \"    if n <= 5000:\\n\",\n",
    "    \"        shapiro_stat, shapiro_p = shapiro(clean_data)\\n\",\n",
    "    \"        results['Shapiro-Wilk'] = {\\n\",\n",
    "    \"            'statistic': shapiro_stat,\\n\",\n",
    "    \"            'p_value': shapiro_p,\\n\",\n",
    "    \"            'is_normal': shapiro_p > alpha\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # D'Agostino-Pearson (bom para amostras grandes)\\n\",\n",
    "    \"    dagostino_stat, dagostino_p = normaltest(clean_data)\\n\",\n",
    "    \"    results['D\\\\'Agostino-Pearson'] = {\\n\",\n",
    "    \"        'statistic': dagostino_stat,\\n\",\n",
    "    \"        'p_value': dagostino_p,\\n\",\n",
    "    \"        'is_normal': dagostino_p > alpha\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Jarque-Bera\\n\",\n",
    "    \"    jb_stat, jb_p = jarque_bera(clean_data)\\n\",\n",
    "    \"    results['Jarque-Bera'] = {\\n\",\n",
    "    \"        'statistic': jb_stat,\\n\",\n",
    "    \"        'p_value': jb_p,\\n\",\n",
    "    \"        'is_normal': jb_p > alpha\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Anderson-Darling\\n\",\n",
    "    \"    ad_result = anderson(clean_data, dist='norm')\\n\",\n",
    "    \"    # Para Œ± = 0.05, usar √≠ndice 2 (5%)\\n\",\n",
    "    \"    ad_critical = ad_result.critical_values[2]  # 5% significance level\\n\",\n",
    "    \"    ad_is_normal = ad_result.statistic < ad_critical\\n\",\n",
    "    \"    results['Anderson-Darling'] = {\\n\",\n",
    "    \"        'statistic': ad_result.statistic,\\n\",\n",
    "    \"        'critical_value': ad_critical,\\n\",\n",
    "    \"        'is_normal': ad_is_normal\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Mostrar resultados\\n\",\n",
    "    \"    for test_name, result in results.items():\\n\",\n",
    "    \"        if test_name == 'Anderson-Darling':\\n\",\n",
    "    \"            print(f\\\"{test_name}:\\\")\\n\",\n",
    "    \"            print(f\\\"  Estat√≠stica: {result['statistic']:.4f}\\\")\\n\",\n",
    "    \"            print(f\\\"  Valor Cr√≠tico (5%): {result['critical_value']:.4f}\\\")\\n\",\n",
    "    \"            print(f\\\"  Normal? {'‚úÖ Sim' if result['is_normal'] else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            print(f\\\"{test_name}:\\\")\\n\",\n",
    "    \"            print(f\\\"  Estat√≠stica: {result['statistic']:.4f}\\\")\\n\",\n",
    "    \"            print(f\\\"  p-valor: {result['p_value']:.6f}\\\")\\n\",\n",
    "    \"            print(f\\\"  Normal? {'‚úÖ Sim' if result['is_normal'] else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"        print()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Conclus√£o\\n\",\n",
    "    \"    normal_tests = sum([r['is_normal'] for r in results.values()])\\n\",\n",
    "    \"    total_tests = len(results)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if normal_tests == total_tests:\\n\",\n",
    "    \"        conclusion = \\\"‚úÖ NORMAL - Todos os testes indicam normalidade\\\"\\n\",\n",
    "    \"    elif normal_tests == 0:\\n\",\n",
    "    \"        conclusion = \\\"‚ùå N√ÉO NORMAL - Nenhum teste indica normalidade\\\"\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        conclusion = f\\\"‚ö†Ô∏è INCONCLUSIVO - {normal_tests}/{total_tests} testes indicam normalidade\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"üéØ CONCLUS√ÉO: {conclusion}\\\")\\n\",\n",
    "    \"    print(\\\"\\\\n\\\" + \\\"=\\\"*60 + \\\"\\\\n\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Testar normalidade das vari√°veis principais\\n\",\n",
    "    \"variables_to_test = {\\n\",\n",
    "    \"    'price': df['price'],\\n\",\n",
    "    \"    'duration': df['duration'],\\n\",\n",
    "    \"    'days_left': df['days_left']\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"normality_results = {}\\n\",\n",
    "    \"for var_name, var_data in variables_to_test.items():\\n\",\n",
    "    \"    normality_results[var_name] = test_normality(var_data, var_name)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualiza√ß√£o da normalidade\\n\",\n",
    "    \"fig, axes = plt.subplots(3, 2, figsize=(15, 15))\\n\",\n",
    "    \"fig.suptitle('An√°lise de Normalidade - Q-Q Plots e Histogramas', fontsize=16, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, (var_name, var_data) in enumerate(variables_to_test.items()):\\n\",\n",
    "    \"    # Q-Q plot\\n\",\n",
    "    \"    stats.probplot(var_data, dist=\\\"norm\\\", plot=axes[i, 0])\\n\",\n",
    "    \"    axes[i, 0].set_title(f'Q-Q Plot - {var_name.title()}')\\n\",\n",
    "    \"    axes[i, 0].grid(True)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Histograma com curva normal\\n\",\n",
    "    \"    axes[i, 1].hist(var_data, bins=30, density=True, alpha=0.7, color=f'C{i}')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Curva normal te√≥rica\\n\",\n",
    "    \"    mu, sigma = var_data.mean(), var_data.std()\\n\",\n",
    "    \"    x = np.linspace(var_data.min(), var_data.max(), 100)\\n\",\n",
    "    \"    axes[i, 1].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normal Te√≥rica')\\n\",\n",
    "    \"    axes[i, 1].set_title(f'Histograma vs Normal - {var_name.title()}')\\n\",\n",
    "    \"    axes[i, 1].legend()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üîó An√°lise de Correla√ß√µes\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Matriz de correla√ß√£o completa\\n\",\n",
    "    \"print(\\\"üîó AN√ÅLISE DE CORRELA√á√ïES\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Selecionar apenas vari√°veis num√©ricas\\n\",\n",
    "    \"numeric_cols = ['duration', 'days_left', 'price']\\n\",\n",
    "    \"correlation_matrix = df[numeric_cols].corr()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üìä Matriz de Correla√ß√£o de Pearson:\\\")\\n\",\n",
    "    \"print(correlation_matrix.round(3))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Teste de signific√¢ncia das correla√ß√µes\\n\",\n",
    "    \"def correlation_significance(df, col1, col2, alpha=0.05):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Testa signific√¢ncia da correla√ß√£o entre duas vari√°veis\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Remover NaNs\\n\",\n",
    "    \"    data1 = df[col1].dropna()\\n\",\n",
    "    \"    data2 = df[col2].dropna()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Garantir que ambos os arrays tenham o mesmo tamanho\\n\",\n",
    "    \"    common_idx = df[[col1, col2]].dropna().index\\n\",\n",
    "    \"    data1 = df.loc[common_idx, col1]\\n\",\n",
    "    \"    data2 = df.loc[common_idx, col2]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Pearson\\n\",\n",
    "    \"    pearson_r, pearson_p = pearsonr(data1, data2)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Spearman (n√£o-param√©trico)\\n\",\n",
    "    \"    spearman_r, spearman_p = spearmanr(data1, data2)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'pearson': {'r': pearson_r, 'p': pearson_p, 'significant': pearson_p < alpha},\\n\",\n",
    "    \"        'spearman': {'r': spearman_r, 'p': spearman_p, 'significant': spearman_p < alpha}\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Testar todas as combina√ß√µes de correla√ß√µes\\n\",\n",
    "    \"correlation_tests = {}\\n\",\n",
    "    \"for i, col1 in enumerate(numeric_cols):\\n\",\n",
    "    \"    for j, col2 in enumerate(numeric_cols):\\n\",\n",
    "    \"        if i < j:  # Evitar duplicatas\\n\",\n",
    "    \"            pair = f\\\"{col1}_vs_{col2}\\\"\\n\",\n",
    "    \"            correlation_tests[pair] = correlation_significance(df, col1, col2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüéØ TESTES DE SIGNIFIC√ÇNCIA DAS CORRELA√á√ïES\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for pair, results in correlation_tests.items():\\n\",\n",
    "    \"    col1, col2 = pair.split('_vs_')\\n\",\n",
    "    \"    print(f\\\"\\\\n{col1.upper()} vs {col2.upper()}:\\\")\\n\",\n",
    "    \"    print(f\\\"  Pearson: r = {results['pearson']['r']:.4f}, p = {results['pearson']['p']:.6f} {'‚úÖ' if results['pearson']['significant'] else '‚ùå'}\\\")\\n\",\n",
    "    \"    print(f\\\"  Spearman: œÅ = {results['spearman']['r']:.4f}, p = {results['spearman']['p']:.6f} {'‚úÖ' if results['spearman']['significant'] else '‚ùå'}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Interpreta√ß√£o da for√ßa da correla√ß√£o\\n\",\n",
    "    \"    r = abs(results['pearson']['r'])\\n\",\n",
    "    \"    if r < 0.1:\\n\",\n",
    "    \"        strength = \\\"neglig√≠vel\\\"\\n\",\n",
    "    \"    elif r < 0.3:\\n\",\n",
    "    \"        strength = \\\"fraca\\\"\\n\",\n",
    "    \"    elif r < 0.5:\\n\",\n",
    "    \"        strength = \\\"moderada\\\"\\n\",\n",
    "    \"    elif r < 0.7:\\n\",\n",
    "    \"        strength = \\\"forte\\\"\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        strength = \\\"muito forte\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"  Interpreta√ß√£o: Correla√ß√£o {strength}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualiza√ß√£o da matriz de correla√ß√£o\\n\",\n",
    "    \"plt.figure(figsize=(10, 8))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Heatmap com anota√ß√µes\\n\",\n",
    "    \"mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\\n\",\n",
    "    \"sns.heatmap(correlation_matrix, \\n\",\n",
    "    \"            mask=mask,\\n\",\n",
    "    \"            annot=True, \\n\",\n",
    "    \"            cmap='RdBu_r', \\n\",\n",
    "    \"            center=0,\\n\",\n",
    "    \"            square=True,\\n\",\n",
    "    \"            fmt='.3f',\\n\",\n",
    "    \"            cbar_kws={\\\"shrink\\\": .8})\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.title('Matriz de Correla√ß√£o - Vari√°veis Num√©ricas', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Scatter plots das correla√ß√µes principais\\n\",\n",
    "    \"fig = make_subplots(\\n\",\n",
    "    \"    rows=1, cols=2,\\n\",\n",
    "    \"    subplot_titles=('Pre√ßo vs Dura√ß√£o', 'Pre√ßo vs Dias Restantes')\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Scatter plot pre√ßo vs dura√ß√£o\\n\",\n",
    "    \"fig.add_trace(\\n\",\n",
    "    \"    go.Scatter(x=df['duration'], y=df['price'], \\n\",\n",
    "    \"               mode='markers', \\n\",\n",
    "    \"               name='Pre√ßo vs Dura√ß√£o',\\n\",\n",
    "    \"               text=df['airline'],\\n\",\n",
    "    \"               marker=dict(size=8, opacity=0.6)),\\n\",\n",
    "    \"    row=1, col=1\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Scatter plot pre√ßo vs dias_left\\n\",\n",
    "    \"fig.add_trace(\\n\",\n",
    "    \"    go.Scatter(x=df['days_left'], y=df['price'], \\n\",\n",
    "    \"               mode='markers', \\n\",\n",
    "    \"               name='Pre√ßo vs Dias',\\n\",\n",
    "    \"               text=df['airline'],\\n\",\n",
    "    \"               marker=dict(size=8, opacity=0.6)),\\n\",\n",
    "    \"    row=1, col=2\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig.update_layout(height=500, title_text=\\\"An√°lise de Correla√ß√µes - Scatter Plots\\\")\\n\",\n",
    "    \"fig.update_xaxes(title_text=\\\"Dura√ß√£o (horas)\\\", row=1, col=1)\\n\",\n",
    "    \"fig.update_xaxes(title_text=\\\"Dias Restantes\\\", row=1, col=2)\\n\",\n",
    "    \"fig.update_yaxes(title_text=\\\"Pre√ßo (‚Çπ)\\\", row=1, col=1)\\n\",\n",
    "    \"fig.update_yaxes(title_text=\\\"Pre√ßo (‚Çπ)\\\", row=1, col=2)\\n\",\n",
    "    \"fig.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## ‚úàÔ∏è Compara√ß√£o Entre Companhias A√©reas\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# ANOVA - Teste se h√° diferen√ßa significativa entre m√©dias de pre√ßos das airlines\\n\",\n",
    "    \"print(\\\"‚úàÔ∏è AN√ÅLISE ESTAT√çSTICA - PRE√áOS POR COMPANHIA A√âREA\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Preparar dados por airline\\n\",\n",
    "    \"airline_groups = [df[df['airline'] == airline]['price'].values for airline in df['airline'].unique()]\\n\",\n",
    "    \"airline_names = df['airline'].unique()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Teste de homogeneidade das vari√¢ncias (Levene)\\n\",\n",
    "    \"levene_stat, levene_p = levene(*airline_groups)\\n\",\n",
    "    \"print(f\\\"üîç TESTE DE LEVENE (Homogeneidade das Vari√¢ncias):\\\")\\n\",\n",
    "    \"print(f\\\"   Estat√≠stica: {levene_stat:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"   p-valor: {levene_p:.6f}\\\")\\n\",\n",
    "    \"print(f\\\"   Vari√¢ncias homog√™neas? {'‚úÖ Sim' if levene_p > 0.05 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Bartlett test (mais sens√≠vel √† normalidade)\\n\",\n",
    "    \"bartlett_stat, bartlett_p = bartlett(*airline_groups)\\n\",\n",
    "    \"print(f\\\"\\\\nüîç TESTE DE BARTLETT (Homogeneidade - sens√≠vel √† normalidade):\\\")\\n\",\n",
    "    \"print(f\\\"   Estat√≠stica: {bartlett_stat:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"   p-valor: {bartlett_p:.6f}\\\")\\n\",\n",
    "    \"print(f\\\"   Vari√¢ncias homog√™neas? {'‚úÖ Sim' if bartlett_p > 0.05 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ANOVA ou Kruskal-Wallis dependendo da normalidade e homogeneidade\\n\",\n",
    "    \"print(f\\\"\\\\nüìä TESTE DE DIFEREN√áA ENTRE GRUPOS:\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ANOVA param√©trico\\n\",\n",
    "    \"f_stat, f_p = f_oneway(*airline_groups)\\n\",\n",
    "    \"print(f\\\"   ANOVA F-test:\\\")\\n\",\n",
    "    \"print(f\\\"     F-estat√≠stica: {f_stat:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"     p-valor: {f_p:.6f}\\\")\\n\",\n",
    "    \"print(f\\\"     Diferen√ßa significativa? {'‚úÖ Sim' if f_p < 0.05 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Kruskal-Wallis n√£o-param√©trico\\n\",\n",
    "    \"kw_stat, kw_p = kruskal(*airline_groups)\\n\",\n",
    "    \"print(f\\\"   Kruskal-Wallis:\\\")\\n\",\n",
    "    \"print(f\\\"     H-estat√≠stica: {kw_stat:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"     p-valor: {kw_p:.6f}\\\")\\n\",\n",
    "    \"print(f\\\"     Diferen√ßa significativa? {'‚úÖ Sim' if kw_p < 0.05 else '‚ùå N√£o'}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Se houver diferen√ßa significativa, fazer teste post-hoc\\n\",\n",
    "    \"if f_p < 0.05:\\n\",\n",
    "    \"    print(f\\\"\\\\nüéØ TESTE POST-HOC (Tukey HSD):\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Preparar dados para Tukey\\n\",\n",
    "    \"    tukey_data = []\\n\",\n",
    "    \"    tukey_labels = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for airline in df['airline'].unique():\\n\",\n",
    "    \"        prices = df[df['airline'] == airline]['price'].values\\n\",\n",
    "    \"        tukey_data.extend(prices)\\n\",\n",
    "    \"        tukey_labels.extend([airline] * len(prices))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    tukey_result = pairwise_tukeyhsd(tukey_data, tukey_labels, alpha=0.05)\\n\",\n",
    "    \"    print(tukey_result)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Estat√≠sticas descritivas por airline\\n\",\n",
    "    \"print(\\\"üìä ESTAT√çSTICAS DESCRITIVAS POR COMPANHIA A√âREA\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"airline_stats = df.groupby('airline')['price'].agg([\\n\",\n",
    "    \"    'count', 'mean', 'std', 'min', 'max', 'median',\\n\",\n",
    "    \"    lambda x: x.quantile(0.25),  # Q1\\n\",\n",
    "    \"    lambda x: x.quantile(0.75)   # Q3\\n\",\n",
    "    \"]).round(0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"airline_stats.columns = ['Count', 'Mean', 'Std', 'Min', 'Max', 'Median', 'Q1', 'Q3']\\n\",\n",
    "    \"airline_stats = airline_stats.sort_values('Mean', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calcular coeficiente de varia√ß√£o\\n\",\n",
    "    \"airline_stats['CV'] = (airline_stats['Std'] / airline_stats['Mean'] * 100).round(1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calcular IQR\\n\",\n",
    "    \"airline_stats['IQR'] = airline_stats['Q3'] - airline_stats['Q1']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(airline_stats)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìà COEFICIENTE DE VARIA√á√ÉO (Std/Mean √ó 100):\\\")\\n\",\n",
    "    \"cv_sorted = airline_stats['CV'].sort_values()\\n\",\n",
    "    \"for airline, cv in cv_sorted.items():\\n\",\n",
    "    \"    stability = \\\"est√°vel\\\" if cv < 20 else \\\"moderada\\\" if cv < 40 else \\\"alta\\\"\\n\",\n",
    "    \"    print(f\\\"   {airline}: {cv}% - Variabilidade {stability}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# An√°lise de efici√™ncia (pre√ßo por hora de voo)\\n\",\n",
    "    \"print(f\\\"\\\\n‚ö° AN√ÅLISE DE EFICI√äNCIA (Pre√ßo por Hora):\\\")\\n\",\n",
    "    \"efficiency_stats = df.groupby('airline').apply(lambda x: (x['price'] / x['duration']).mean()).sort_values()\\n\",\n",
    "    \"for airline, efficiency in efficiency_stats.items():\\n\",\n",
    "    \"    print(f\\\"   {airline}: ‚Çπ{efficiency:.0f}/hora\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## ‚è∞ An√°lise Temporal - Hor√°rios de Partida\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    # An√°lise estat√≠stica dos pre√ßos por hor√°rio de partida\n",
    "print(\"üïê AN√ÅLISE ESTAT√çSTICA - PRE√áOS POR HOR√ÅRIO DE PARTIDA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Preparar dados por hor√°rio\n",
    "time_groups = [df[df['departure_time'] == time]['price'].values for time in df['departure_time'].unique()]\n",
    "time_names = df['departure_time'].unique()\n",
    "\n",
    "# Teste de homogeneidade das vari√¢ncias\n",
    "levene_stat_time, levene_p_time = levene(*time_groups)\n",
    "print(f\"üîç TESTE DE LEVENE (Homogeneidade das Vari√¢ncias):\")\n",
    "print(f\"   Estat√≠stica: {levene_stat_time:.4f}\")\n",
    "print(f\"   p-valor: {levene_p_time:.6f}\")\n",
    "print(f\"   Vari√¢ncias homog√™neas? {'‚úÖ Sim' if levene_p_time > 0.05 else '‚ùå N√£o'}\")\n",
    "\n",
    "# ANOVA para hor√°rios de partida\n",
    "f_stat_time, f_p_time = f_oneway(*time_groups)\n",
    "print(f\"\\nüìä ANOVA - PRE√áOS POR HOR√ÅRIO:\")\n",
    "print(f\"   F-estat√≠stica: {f_stat_time:.4f}\")\n",
    "print(f\"   p-valor: {f_p_time:.6f}\")\n",
    "print(f\"   Diferen√ßa significativa? {'‚úÖ Sim' if f_p_time < 0.05 else '‚ùå N√£o'}\")\n",
    "\n",
    "# Kruskal-Wallis (n√£o-param√©trico)\n",
    "kw_stat_time, kw_p_time = kruskal(*time_groups)\n",
    "print(f\"\\n   Kruskal-Wallis:\")\n",
    "print(f\"   H-estat√≠stica: {kw_stat_time:.4f}\")\n",
    "print(f\"   p-valor: {kw_p_time:.6f}\")\n",
    "print(f\"   Diferen√ßa significativa? {'‚úÖ Sim' if kw_p_time < 0.05 else '‚ùå N√£o'}\")\n",
    "\n",
    "# Post-hoc test se significativo\n",
    "if f_p_time < 0.05:\n",
    "    print(f\"\\nüéØ TESTE POST-HOC (Tukey HSD) - HOR√ÅRIOS:\")\n",
    "    \n",
    "    # Preparar dados para Tukey\n",
    "    time_data = []\n",
    "    time_labels = []\n",
    "    \n",
    "    for time in df['departure_time'].unique():\n",
    "        prices = df[df['departure_time'] == time]['price'].values\n",
    "        time_data.extend(prices)\n",
    "        time_labels.extend([time] * len(prices))\n",
    "    \n",
    "    tukey_time_result = pairwise_tukeyhsd(time_data, time_labels, alpha=0.05)\n",
    "    print(tukey_time_result)\n",
    "\n",
    "## üõë An√°lise de Paradas (Stops) - Teste T\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üõë AN√ÅLISE ESTAT√çSTICA - VOOS DIRETOS vs COM PARADAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Separar grupos\n",
    "direct_flights = df[df['stops'] == 'zero']['price'].values\n",
    "flights_with_stops = df[df['stops'] == 'one']['price'].values\n",
    "\n",
    "print(f\"üìä ESTAT√çSTICAS B√ÅSICAS:\")\n",
    "print(f\"   Voos diretos: n={len(direct_flights)}, m√©dia=‚Çπ{np.mean(direct_flights):.0f}, std=‚Çπ{np.std(direct_flights):.0f}\")\n",
    "print(f\"   Voos com paradas: n={len(flights_with_stops)}, m√©dia=‚Çπ{np.mean(flights_with_stops):.0f}, std=‚Çπ{np.std(flights_with_stops):.0f}\")\n",
    "\n",
    "# Teste de homogeneidade das vari√¢ncias\n",
    "levene_stat_stops, levene_p_stops = levene(direct_flights, flights_with_stops)\n",
    "print(f\"\\nüîç TESTE DE LEVENE:\")\n",
    "print(f\"   Estat√≠stica: {levene_stat_stops:.4f}\")\n",
    "print(f\"   p-valor: {levene_p_stops:.6f}\")\n",
    "print(f\"   Vari√¢ncias homog√™neas? {'‚úÖ Sim' if levene_p_stops > 0.05 else '‚ùå N√£o'}\")\n",
    "\n",
    "# Teste t independente\n",
    "equal_var = levene_p_stops > 0.05\n",
    "t_stat, t_p = ttest_ind(direct_flights, flights_with_stops, equal_var=equal_var)\n",
    "print(f\"\\nüìä TESTE T INDEPENDENTE:\")\n",
    "print(f\"   t-estat√≠stica: {t_stat:.4f}\")\n",
    "print(f\"   p-valor: {t_p:.6f}\")\n",
    "print(f\"   Diferen√ßa significativa? {'‚úÖ Sim' if t_p < 0.05 else '‚ùå N√£o'}\")\n",
    "\n",
    "# Mann-Whitney U (n√£o-param√©trico)\n",
    "u_stat, u_p = mannwhitneyu(direct_flights, flights_with_stops, alternative='two-sided')\n",
    "print(f\"\\n   Mann-Whitney U:\")\n",
    "print(f\"   U-estat√≠stica: {u_stat:.4f}\")\n",
    "print(f\"   p-valor: {u_p:.6f}\")\n",
    "print(f\"   Diferen√ßa significativa? {'‚úÖ Sim' if u_p < 0.05 else '‚ùå N√£o'}\")\n",
    "\n",
    "# C√°lculo do tamanho do efeito (Cohen's d)\n",
    "def cohens_d(x, y):\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    dof = nx + ny - 2\n",
    "    pooled_std = np.sqrt(((nx-1)*np.std(x, ddof=1)**2 + (ny-1)*np.std(y, ddof=1)**2) / dof)\n",
    "    return (np.mean(x) - np.mean(y)) / pooled_std\n",
    "\n",
    "effect_size = cohens_d(direct_flights, flights_with_stops)\n",
    "print(f\"\\nüìè TAMANHO DO EFEITO (Cohen's d): {effect_size:.3f}\")\n",
    "if abs(effect_size) < 0.2:\n",
    "    effect_interpretation = \"pequeno\"\n",
    "elif abs(effect_size) < 0.5:\n",
    "    effect_interpretation = \"m√©dio\"\n",
    "elif abs(effect_size) < 0.8:\n",
    "    effect_interpretation = \"grande\"\n",
    "else:\n",
    "    effect_interpretation = \"muito grande\"\n",
    "print(f\"   Interpreta√ß√£o: Efeito {effect_interpretation}\")\n",
    "\n",
    "## üìà Modelagem de Regress√£o Linear\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà AN√ÅLISE DE REGRESS√ÉO LINEAR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Preparar vari√°veis para regress√£o\n",
    "X = df[['duration', 'days_left']].copy()\n",
    "y = df['price'].copy()\n",
    "\n",
    "# Adicionar vari√°veis dummy para categ√≥ricas\n",
    "airline_dummies = pd.get_dummies(df['airline'], prefix='airline')\n",
    "stops_dummies = pd.get_dummies(df['stops'], prefix='stops')\n",
    "time_dummies = pd.get_dummies(df['departure_time'], prefix='time')\n",
    "\n",
    "# Combinar todas as vari√°veis (removendo uma categoria de cada para evitar multicolinearidade)\n",
    "X_full = pd.concat([\n",
    "    X,\n",
    "    airline_dummies.iloc[:, :-1],  # Remove last airline\n",
    "    stops_dummies.iloc[:, :-1],    # Remove last stop category\n",
    "    time_dummies.iloc[:, :-1]      # Remove last time category\n",
    "], axis=1)\n",
    "\n",
    "# Adicionar constante\n",
    "X_full_const = sm.add_constant(X_full)\n",
    "\n",
    "# Ajustar modelo\n",
    "model = sm.OLS(y, X_full_const).fit()\n",
    "\n",
    "print(\"üîç RESULTADOS DA REGRESS√ÉO:\")\n",
    "print(model.summary())\n",
    "\n",
    "# Testes de diagn√≥stico do modelo\n",
    "print(f\"\\nüî¨ DIAGN√ìSTICOS DO MODELO:\")\n",
    "print(f\"   R¬≤ Ajustado: {model.rsquared_adj:.4f}\")\n",
    "print(f\"   AIC: {model.aic:.2f}\")\n",
    "print(f\"   BIC: {model.bic:.2f}\")\n",
    "print(f\"   F-statistic: {model.fvalue:.2f} (p={model.f_pvalue:.6f})\")\n",
    "\n",
    "# Teste de heterocedasticidade (Breusch-Pagan)\n",
    "bp_stat, bp_p, bp_f_stat, bp_f_p = het_breuschpagan(model.resid, model.model.exog)\n",
    "print(f\"\\nüîç TESTE DE HETEROCEDASTICIDADE (Breusch-Pagan):\")\n",
    "print(f\"   Estat√≠stica: {bp_stat:.4f}\")\n",
    "print(f\"   p-valor: {bp_p:.6f}\")\n",
    "print(f\"   Homocedasticidade? {'‚úÖ Sim' if bp_p > 0.05 else '‚ùå N√£o'}\")\n",
    "\n",
    "# An√°lise de res√≠duos\n",
    "residuals = model.resid\n",
    "print(f\"\\nüìä AN√ÅLISE DE RES√çDUOS:\")\n",
    "print(f\"   M√©dia dos res√≠duos: {np.mean(residuals):.6f}\")\n",
    "print(f\"   Desvio padr√£o dos res√≠duos: {np.std(residuals):.2f}\")\n",
    "\n",
    "# Teste de normalidade dos res√≠duos\n",
    "shapiro_resid_stat, shapiro_resid_p = shapiro(residuals)\n",
    "print(f\"   Normalidade dos res√≠duos (Shapiro): p={shapiro_resid_p:.6f} {'‚úÖ' if shapiro_resid_p > 0.05 else '‚ùå'}\")\n",
    "\n",
    "# Visualiza√ß√£o dos diagn√≥sticos da regress√£o\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Diagn√≥sticos da Regress√£o Linear', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Res√≠duos vs Valores preditos\n",
    "axes[0, 0].scatter(model.fittedvalues, residuals, alpha=0.6)\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Valores Preditos')\n",
    "axes[0, 0].set_ylabel('Res√≠duos')\n",
    "axes[0, 0].set_title('Res√≠duos vs Valores Preditos')\n",
    "\n",
    "# Q-Q plot dos res√≠duos\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Q-Q Plot - Res√≠duos')\n",
    "\n",
    "# Histograma dos res√≠duos\n",
    "axes[1, 0].hist(residuals, bins=30, density=True, alpha=0.7)\n",
    "mu, sigma = np.mean(residuals), np.std(residuals)\n",
    "x = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "axes[1, 0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Res√≠duos')\n",
    "axes[1, 0].set_ylabel('Densidade')\n",
    "axes[1, 0].set_title('Distribui√ß√£o dos Res√≠duos')\n",
    "\n",
    "# Scale-Location plot\n",
    "standardized_resid = np.sqrt(np.abs(stats.zscore(residuals)))\n",
    "axes[1, 1].scatter(model.fittedvalues, standardized_resid, alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Valores Preditos')\n",
    "axes[1, 1].set_ylabel('‚àö|Res√≠duos Padronizados|')\n",
    "axes[1, 1].set_title('Scale-Location Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## üéØ An√°lise de Clustering\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ AN√ÅLISE DE CLUSTERING - SEGMENTA√á√ÉO DE MERCADO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Preparar dados para clustering\n",
    "clustering_data = df[['price', 'duration', 'days_left']].copy()\n",
    "\n",
    "# Padronizar os dados\n",
    "scaler = StandardScaler()\n",
    "clustering_scaled = scaler.fit_transform(clustering_data)\n",
    "\n",
    "# Determinar n√∫mero √≥timo de clusters usando m√©todo do cotovelo\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(clustering_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(clustering_scaled, kmeans.labels_))\n",
    "\n",
    "# Visualizar m√©trica do cotovelo e silhueta\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(K_range, inertias, 'bo-')\n",
    "axes[0].set_xlabel('N√∫mero de Clusters (k)')\n",
    "axes[0].set_ylabel('In√©rcia')\n",
    "axes[0].set_title('M√©todo do Cotovelo')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(K_range, silhouette_scores, 'ro-')\n",
    "axes[1].set_xlabel('N√∫mero de Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('An√°lise de Silhueta')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Escolher n√∫mero √≥timo de clusters (maior silhouette score)\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f\"üéØ N√öMERO √ìTIMO DE CLUSTERS: {optimal_k}\")\n",
    "print(f\"   Silhouette Score: {max(silhouette_scores):.4f}\")\n",
    "\n",
    "# Aplicar clustering com k √≥timo\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_optimal.fit_predict(clustering_scaled)\n",
    "\n",
    "# Adicionar labels ao dataframe original\n",
    "df_clustered = df.copy()\n",
    "df_clustered['cluster'] = cluster_labels\n",
    "\n",
    "# Analisar caracter√≠sticas de cada cluster\n",
    "print(f\"\\nüìä AN√ÅLISE DOS CLUSTERS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cluster_summary = df_clustered.groupby('cluster').agg({\n",
    "    'price': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'duration': 'mean',\n",
    "    'days_left': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "cluster_summary.columns = ['Count', 'Price_Mean', 'Price_Std', 'Price_Min', 'Price_Max', 'Duration_Mean', 'DaysLeft_Mean']\n",
    "\n",
    "print(cluster_summary)\n",
    "\n",
    "# An√°lise detalhada de cada cluster\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['cluster'] == cluster_id]\n",
    "    print(f\"\\nüè∑Ô∏è CLUSTER {cluster_id}:\")\n",
    "    print(f\"   Tamanho: {len(cluster_data)} voos ({len(cluster_data)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Pre√ßo m√©dio: ‚Çπ{cluster_data['price'].mean():.0f}\")\n",
    "    print(f\"   Dura√ß√£o m√©dia: {cluster_data['duration'].mean():.1f} horas\")\n",
    "    print(f\"   Airlines principais: {dict(cluster_data['airline'].value_counts().head(3))}\")\n",
    "    print(f\"   Voos diretos: {(cluster_data['stops'] == 'zero').mean()*100:.1f}%\")\n",
    "    \n",
    "    # Caracterizar o cluster\n",
    "    if cluster_data['price'].mean() < df['price'].quantile(0.33):\n",
    "        cluster_type = \"BUDGET\"\n",
    "    elif cluster_data['price'].mean() > df['price'].quantile(0.67):\n",
    "        cluster_type = \"PREMIUM\"\n",
    "    else:\n",
    "        cluster_type = \"MAINSTREAM\"\n",
    "    \n",
    "    print(f\"   Tipo: {cluster_type}\")\n",
    "\n",
    "# Visualiza√ß√£o dos clusters\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 3D scatter plot\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "scatter = ax1.scatter(df_clustered['price'], df_clustered['duration'], df_clustered['days_left'], \n",
    "                     c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "ax1.set_xlabel('Pre√ßo')\n",
    "ax1.set_ylabel('Dura√ß√£o')\n",
    "ax1.set_zlabel('Dias Restantes')\n",
    "ax1.set_title('Clusters 3D')\n",
    "plt.colorbar(scatter, ax=ax1)\n",
    "\n",
    "# 2D scatter plots\n",
    "ax2 = fig.add_subplot(222)\n",
    "scatter2 = ax2.scatter(df_clustered['price'], df_clustered['duration'], \n",
    "                      c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "ax2.set_xlabel('Pre√ßo')\n",
    "ax2.set_ylabel('Dura√ß√£o')\n",
    "ax2.set_title('Clusters: Pre√ßo vs Dura√ß√£o')\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "scatter3 = ax3.scatter(df_clustered['price'], df_clustered['days_left'], \n",
    "                      c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "ax3.set_xlabel('Pre√ßo')\n",
    "ax3.set_ylabel('Dias Restantes')\n",
    "ax3.set_title('Clusters: Pre√ßo vs Dias Restantes')\n",
    "\n",
    "# Distribui√ß√£o dos clusters por airline\n",
    "ax4 = fig.add_subplot(224)\n",
    "cluster_airline = pd.crosstab(df_clustered['airline'], df_clustered['cluster'], normalize='index')\n",
    "cluster_airline.plot(kind='bar', stacked=True, ax=ax4)\n",
    "ax4.set_title('Distribui√ß√£o dos Clusters por Airline')\n",
    "ax4.set_xlabel('Airline')\n",
    "ax4.set_ylabel('Propor√ß√£o')\n",
    "ax4.legend(title='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lise de associa√ß√£o entre vari√°veis categ√≥ricas\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîó AN√ÅLISE DE ASSOCIA√á√ÉO - VARI√ÅVEIS CATEG√ìRICAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Teste Chi-quadrado: Airline vs Stops\n",
    "contingency_airline_stops = pd.crosstab(df['airline'], df['stops'])\n",
    "chi2_stat, chi2_p, chi2_dof, chi2_expected = chi2_contingency(contingency_airline_stops)\n",
    "\n",
    "print(\"‚úàÔ∏è TESTE CHI-QUADRADO: AIRLINE vs STOPS\")\n",
    "print(f\"   Estat√≠stica Chi¬≤: {chi2_stat:.4f}\")\n",
    "print(f\"   p-valor: {chi2_p:.6f}\")\n",
    "print(f\"   Graus de liberdade: {chi2_dof}\")\n",
    "print(f\"   Associa√ß√£o significativa? {'‚úÖ Sim' if chi2_p < 0.05 else '‚ùå N√£o'}\")\n",
    "\n",
    "print(f\"\\nüìä TABELA DE CONTING√äNCIA:\")\n",
    "print(contingency_airline_stops)\n",
    "\n",
    "# Coeficiente de conting√™ncia (for√ßa da associa√ß√£o)\n",
    "n = contingency_airline_stops.sum().sum()\n",
    "contingency_coeff = np.sqrt(chi2_stat / (chi2_stat + n))\n",
    "print(f\"\\nüìè COEFICIENTE DE CONTING√äNCIA: {contingency_coeff:.4f}\")\n",
    "\n",
    "# Teste Chi-quadrado: Departure Time vs Price Range\n",
    "df['price_category'] = pd.qcut(df['price'], q=3, labels=['Baixo', 'M√©dio', 'Alto'])\n",
    "contingency_time_price = pd.crosstab(df['departure_time'], df['price_category'])\n",
    "chi2_time_stat, chi2_time_p, chi2_time_dof, chi2_time_expected = chi2_contingency(contingency_time_price)\n",
    "\n",
    "print(f\"\\nüïê TESTE CHI-QUADRADO: DEPARTURE_TIME vs PRICE_CATEGORY\")\n",
    "print(f\"   Estat√≠stica Chi¬≤: {chi2_time_stat:.4f}\")\n",
    "print(f\"   p-valor: {chi2_time_p:.6f}\")\n",
    "print(f\"   Associa√ß√£o significativa? {'‚úÖ Sim' if chi2_time_p < 0.05 else '‚ùå N√£o'}\")\n",
    "\n",
    "# An√°lise de vari√¢ncia explicada\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà AN√ÅLISE DE VARI√ÇNCIA EXPLICADA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ANOVA para cada vari√°vel categ√≥rica\n",
    "print(\"üîç VARI√ÇNCIA EXPLICADA POR CADA FATOR:\")\n",
    "\n",
    "# Airline\n",
    "airline_groups = [df[df['airline'] == airline]['price'] for airline in df['airline'].unique()]\n",
    "f_airline, p_airline = f_oneway(*airline_groups)\n",
    "eta_squared_airline = (f_airline * (len(df['airline'].unique()) - 1)) / (f_airline * (len(df['airline'].unique()) - 1) + len(df) - len(df['airline'].unique()))\n",
    "\n",
    "print(f\"   Airline: Œ∑¬≤ = {eta_squared_airline:.4f} ({eta_squared_airline*100:.1f}% da vari√¢ncia)\")\n",
    "\n",
    "# Departure Time  \n",
    "time_groups = [df[df['departure_time'] == time]['price'] for time in df['departure_time'].unique()]\n",
    "f_time, p_time = f_oneway(*time_groups)\n",
    "eta_squared_time = (f_time * (len(df['departure_time'].unique()) - 1)) / (f_time * (len(df['departure_time'].unique()) - 1) + len(df) - len(df['departure_time'].unique()))\n",
    "\n",
    "print(f\"   Departure Time: Œ∑¬≤ = {eta_squared_time:.4f} ({eta_squared_time*100:.1f}% da vari√¢ncia)\")\n",
    "\n",
    "# Stops\n",
    "stops_groups = [df[df['stops'] == stop]['price'] for stop in df['stops'].unique()]\n",
    "f_stops, p_stops = f_oneway(*stops_groups)\n",
    "eta_squared_stops = (f_stops * (len(df['stops'].unique()) - 1)) / (f_stops * (len(df['stops'].unique()) - 1) + len(df) - len(df['stops'].unique()))\n",
    "\n",
    "print(f\"   Stops: Œ∑¬≤ = {eta_squared_stops:.4f} ({eta_squared_stops*100:.1f}% da vari√¢ncia)\")\n",
    "\n",
    "# An√°lise de intera√ß√µes\n",
    "print(\"\\nüîó AN√ÅLISE DE INTERA√á√ïES ENTRE FATORES:\")\n",
    "\n",
    "# Intera√ß√£o Airline √ó Stops\n",
    "interaction_data = []\n",
    "for airline in df['airline'].unique():\n",
    "    for stop in df['stops'].unique():\n",
    "        subset = df[(df['airline'] == airline) & (df['stops'] == stop)]\n",
    "        if len(subset) > 0:\n",
    "            interaction_data.append({\n",
    "                'airline': airline,\n",
    "                'stops': stop,\n",
    "                'mean_price': subset['price'].mean(),\n",
    "                'count': len(subset)\n",
    "            })\n",
    "\n",
    "interaction_df = pd.DataFrame(interaction_data)\n",
    "interaction_pivot = interaction_df.pivot(index='airline', columns='stops', values='mean_price')\n",
    "\n",
    "print(\"üí∞ PRE√áO M√âDIO POR AIRLINE √ó STOPS:\")\n",
    "print(interaction_pivot.round(0))\n",
    "\n",
    "# Teste de robustez dos resultados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üõ°Ô∏è TESTES DE ROBUSTEZ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Bootstrap para intervalos de confian√ßa das m√©dias\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "def bootstrap_mean(data, n_bootstrap=1000, confidence=0.95):\n",
    "    \"\"\"Calcula intervalo de confian√ßa bootstrap para a m√©dia\"\"\"\n",
    "    bootstrap_means = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        bootstrap_means.append(np.mean(sample))\n",
    "    \n",
    "    alpha = 1 - confidence\n",
    "    lower = np.percentile(bootstrap_means, (alpha/2) * 100)\n",
    "    upper = np.percentile(bootstrap_means, (1 - alpha/2) * 100)\n",
    "    \n",
    "    return lower, upper\n",
    "\n",
    "print(\"üîÑ INTERVALOS DE CONFIAN√áA BOOTSTRAP (95%):\")\n",
    "print(\"   Pre√ßos por Airline:\")\n",
    "\n",
    "for airline in df['airline'].unique():\n",
    "    airline_prices = df[df['airline'] == airline]['price'].values\n",
    "    if len(airline_prices) > 10:  # Apenas se tiver dados suficientes\n",
    "        ci_lower, ci_upper = bootstrap_mean(airline_prices)\n",
    "        mean_price = np.mean(airline_prices)\n",
    "        print(f\"     {airline}: ‚Çπ{mean_price:.0f} [‚Çπ{ci_lower:.0f}, ‚Çπ{ci_upper:.0f}]\")\n",
    "\n",
    "# An√°lise de sensibilidade - removendo outliers\n",
    "print(f\"\\nüéØ AN√ÅLISE DE SENSIBILIDADE - REMOVENDO OUTLIERS:\")\n",
    "\n",
    "# Identificar outliers usando IQR\n",
    "Q1 = df['price'].quantile(0.25)\n",
    "Q3 = df['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df_no_outliers = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]\n",
    "print(f\"   Dados originais: {len(df)} observa√ß√µes\")\n",
    "print(f\"   Sem outliers: {len(df_no_outliers)} observa√ß√µes\")\n",
    "print(f\"   Outliers removidos: {len(df) - len(df_no_outliers)} ({(len(df) - len(df_no_outliers))/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Recomputar testes principais sem outliers\n",
    "airline_groups_clean = [df_no_outliers[df_no_outliers['airline'] == airline]['price'].values \n",
    "                       for airline in df_no_outliers['airline'].unique()]\n",
    "f_clean, p_clean = f_oneway(*airline_groups_clean)\n",
    "\n",
    "print(f\"\\n   ANOVA sem outliers:\")\n",
    "print(f\"     F-estat√≠stica: {f_clean:.4f} (original: {f_stat:.4f})\")\n",
    "print(f\"     p-valor: {p_clean:.6f} (original: {f_p:.6f})\")\n",
    "print(f\"     Resultado {'mantido' if (p_clean < 0.05) == (f_p < 0.05) else 'alterado'}\")\n",
    "\n",
    "# Resumo executivo dos testes estat√≠sticos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã RESUMO EXECUTIVO - TESTES ESTAT√çSTICOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_results = {\n",
    "    \"Normalidade dos Dados\": {\n",
    "        \"price\": \"N√£o normal (p < 0.001)\" if normality_results['price']['Shapiro-Wilk']['p_value'] < 0.001 else \"Normal\",\n",
    "        \"duration\": \"N√£o normal (p < 0.001)\" if normality_results['duration']['Shapiro-Wilk']['p_value'] < 0.001 else \"Normal\",\n",
    "        \"implicacao\": \"Usar testes n√£o-param√©tricos quando apropriado\"\n",
    "    },\n",
    "    \"Diferen√ßas entre Airlines\": {\n",
    "        \"ANOVA\": f\"F={f_stat:.2f}, p={f_p:.6f}\",\n",
    "        \"significativo\": \"Sim\" if f_p < 0.05 else \"N√£o\",\n",
    "        \"interpretacao\": \"Diferen√ßas significativas entre airlines\" if f_p < 0.05 else \"Sem diferen√ßas significativas\"\n",
    "    },\n",
    "    \"Diferen√ßas entre Hor√°rios\": {\n",
    "        \"ANOVA\": f\"F={f_stat_time:.2f}, p={f_p_time:.6f}\",\n",
    "        \"significativo\": \"Sim\" if f_p_time < 0.05 else \"N√£o\",\n",
    "        \"interpretacao\": \"Diferen√ßas significativas entre hor√°rios\" if f_p_time < 0.05 else \"Sem diferen√ßas significativas\"\n",
    "    },\n",
    "    \"Voos Diretos vs Paradas\": {\n",
    "        \"teste_t\": f\"t={t_stat:.2f}, p={t_p:.6f}\",\n",
    "        \"significativo\": \"Sim\" if t_p < 0.05 else \"N√£o\",\n",
    "        \"tamanho_efeito\": f\"Cohen's d = {effect_size:.3f} (efeito {effect_interpretation})\",\n",
    "        \"interpretacao\": \"Voos diretos significativamente diferentes\" if t_p < 0.05 else \"Sem diferen√ßa significativa\"\n",
    "    },\n",
    "    \"Modelo de Regress√£o\": {\n",
    "        \"R_quadrado\": f\"{model.rsquared:.4f}\",\n",
    "        \"R_quadrado_ajustado\": f\"{model.rsquared_adj:.4f}\",\n",
    "        \"interpretacao\": f\"Modelo explica {model.rsquared_adj*100:.1f}% da vari√¢ncia nos pre√ßos\"\n",
    "    },\n",
    "    \"Segmenta√ß√£o (Clustering)\": {\n",
    "        \"clusters_otimos\": optimal_k,\n",
    "        \"silhouette_score\": f\"{max(silhouette_scores):.4f}\",\n",
    "        \"interpretacao\": \"Segmenta√ß√£o clara do mercado identificada\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, results in summary_results.items():\n",
    "    print(f\"\\nüéØ {category}:\")\n",
    "    for key, value in results.items():\n",
    "        if key != \"interpretacao\":\n",
    "            print(f\"   {key}: {value}\")\n",
    "    print(f\"   ‚Üí {results.get('interpretacao', '')}\")\n",
    "\n",
    "# Exportar resultados da an√°lise estat√≠stica\n",
    "statistical_results = {\n",
    "    \"normality_tests\": normality_results,\n",
    "    \"correlation_analysis\": correlation_tests,\n",
    "    \"anova_results\": {\n",
    "        \"airlines\": {\"F\": f_stat, \"p\": f_p, \"significant\": f_p < 0.05},\n",
    "        \"departure_times\": {\"F\": f_stat_time, \"p\": f_p_time, \"significant\": f_p_time < 0.05},\n",
    "        \"stops\": {\"t\": t_stat, \"p\": t_p, \"significant\": t_p < 0.05, \"cohens_d\": effect_size}\n",
    "    },\n",
    "    \"regression_model\": {\n",
    "        \"r_squared\": model.rsquared,\n",
    "        \"r_squared_adj\": model.rsquared_adj,\n",
    "        \"aic\": model.aic,\n",
    "        \"bic\": model.bic\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"optimal_clusters\": optimal_k,\n",
    "        \"silhouette_score\": max(silhouette_scores)\n",
    "    },\n",
    "    \"market_segmentation\": cluster_summary.to_dict(),\n",
    "    \"recommendations\": [\n",
    "        \"Diferen√ßas significativas entre airlines justificam estrat√©gias diferenciadas\",\n",
    "        \"Hor√°rios de partida influenciam pre√ßos - oportunidade de otimiza√ß√£o\",\n",
    "        f\"Voos diretos t√™m premium de ‚Çπ{abs(np.mean(direct_flights) - np.mean(flights_with_stops)):.0f}\",\n",
    "        f\"Mercado naturalmente segmentado em {optimal_k} grupos distintos\",\n",
    "        f\"Modelo preditivo explica {model.rsquared_adj*100:.1f}% da varia√ß√£o nos pre√ßos\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Salvar resultados\n",
    "import json\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "with open('../data/processed/statistical_analysis_results.json', 'w') as f:\n",
    "    # Converter numpy types para tipos serializ√°veis\n",
    "    def convert_numpy(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return obj\n",
    "    \n",
    "    json.dump(statistical_results, f, indent=2, default=convert_numpy)\n",
    "\n",
    "# Salvar dataset com clusters\n",
    "df_clustered.to_csv('../data/processed/flights_with_clusters.csv', index=False)\n",
    "\n",
    "print(f\"\\nüíæ RESULTADOS SALVOS:\")\n",
    "print(f\"   üìä An√°lise estat√≠stica: ../data/processed/statistical_analysis_results.json\")\n",
    "print(f\"   üéØ Dataset com clusters: ../data/processed/flights_with_clusters.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ AN√ÅLISE ESTAT√çSTICA CONCLU√çDA COM SUCESSO!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüöÄ PR√ìXIMOS PASSOS:\")\n",
    "print(\"   1. Desenvolver modelos de machine learning\")\n",
    "print(\"   2. Criar dashboard interativo em Streamlit\")\n",
    "print(\"   3. Implementar sistema de recomenda√ß√µes\")\n",
    "print(\"   4. Validar insights com dados externos\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
